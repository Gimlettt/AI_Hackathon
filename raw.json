[
    {
        "assignment_name": "3F7 EP",
        "DDL": "1/1",
        "content": "3F7: Information Theory and Coding (Michaelmas 2023)\nExamples Paper 2\n1.Limiting behaviour of the log-likelihood ratio : Let P, Q be two pmfs defined on the same alphabet. Given\ndata X1, . . . , X n, the log-likelihood ratio (LLR) is defined as\nLLR (X1, . . . , X n) =1\nnlogQ(X1, . . . , X n)\nP(X1, . . . , X n),\nwhere Q(X1, . . . , X n) =Q\niQ(Xi) and P(X1, . . . , X n) =Q\niP(Xi).\n(a) Using the Weak Law of Large Numbers, show that the LLR converges to D(Q\u2225P) when the data are\ngenerated i.i.d. according to Q.\n(b) Show that the LLR converges to \u2212D(P\u2225Q) when the data are generated i.i.d. according to P.\nHence argue that the likelihood of the data under the wrong distribution is exponentially smaller than the\nlikelihood under the true distribution, with the relative entropy specifying the exponent.\n2.Relative Entropy between Gaussians and Hypothesis Testing : Let P, Q be unit variance Gaussian distribu-\ntions with different means: P\u223c N(\u00b51,1) and Q\u223c N(\u00b52,1).\n(a) Compute the relative entropy D(P\u2225Q). (Note that the relative entropy between continuous distribu-\ntions with densities pandqis defined as D(p\u2225q) =R\u221e\n\u2212\u221ep(x) logp(x)\nq(x)dx.)\n(b) Consider a hypothesis testing problem, where under hypothesis H0, the data X1, . . . , X nwere gener-\nated i.i.d. from P, and under hypothesis H1, the data were generated i.i.d. from Q. The distributions\nPandQare the Gaussians defined above. Write down an expression for log-likelihood ratio (LLR)\n1\nnlogQ(X1,...,X n)\nP(X1,...,X n)and simplify.\n(c) Using the law of large numbers, verify that that the LLR expression you obtained in part (b) converges\ntoD(Q\u2225P) under H1and to \u2212D(P\u2225Q) under H0.\n3.Convex and concave functions : A function is called convex if it always lies below any line segment joining\ntwo points on the function. Mathematically, a function fdefined on an interval ( a, b) is convex if for every\nx1, x2\u2208(a, b) and 0 \u2264\u03bb\u22641, we have\nf(\u03bbx1+ (1\u2212\u03bb)x2)\u2264\u03bbf(x1) + (1 \u2212\u03bb)f(x2).\nThe function is strictly convex if equality holds only if \u03bb= 0 or \u03bb= 1, i.e., if the function lies strictly below\nany line segment. A function fis called concave if\u2212fis convex.\nLabel the following functions as convex, concave, or neither: x2,logx,\u221ax, all on the domain (0 ,\u221e), and\nthe following function on the domain ( \u2212\u221e,\u221e):\nf(x) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3x\u22121,forx >1,\n0 for \u22121\u2264x\u22641,\nx+ 1,forx <\u22121.\nYou can do this question by examining the graph of each function. For a function that can be differentiated\ntwice, it can be shown that it is convex if the second derivative is non-negative (and strictly convex if the\nsecond derivative is strictly positive).\n4.Jensen\u2019s Inequality : Iffis a function that is convex on an interval, and Xis a random variable which takes\nvalues within the interval, then Jensen\u2019s inequality says:\nE\u0002\nf(X)\u0003\n\u2265f(E[X]).\n(a) Prove Jensen\u2019s inequality for the case where Xis a binary random variable that values x1andx2\nwith probabilities pand (1 \u2212p). The function is convex in an interval containing x1, x2. (Hint: Write\ndown the expression for the expectation on each side, and notice that the inequality follows from the\ndefinition of convexity.)\n1(b) Jensen\u2019s inequality holds for any random variable (not just binary) which takes values within the domain\nof the convex function. By using the inequality for the convex function f(x) =\u2212log2(x), show that\nD(P\u2225Q)\u22650 for any ( Hint: Start with \u2212D(P\u2225Q)) as in the lecture notes, but use Jensen\u2019s inequality\ninstead of ln( x)\u2264(x\u22121).)\nComment : A proof of Jensen\u2019s inequality for general alphabets can be found in the book by Cover\nand Thomas (Theorem 2.6.2). Several information-theoretic inequalities can be derived using Jensen\u2019s\ninequality.\n5.Inequalities : Which of the following inequalities are generally \u2265, =,\u2264? Label each with \u2265, =, or \u2264.\n(a)H(5X) vs. H(X)\n(b)H(X1|X0) vsH(X1|X0, X2)\n(c)H(X, Y ) vs. H(X) +H(Y)\n(d)I(g(X);Y) vs. I(X;Y) (Hint: Consider I(X, g(X);Y) and expand using the chain rule for mutual\ninformation in two different ways. )\n6.Chain Rule for Mutual Information : For any sequence of random variables X1, X2, . . . , X njointly dis-\ntributed with Y, show that the mutual information can be expressed as\nI(X1, X2, . . . , X n;Y) =nX\ni=1I(Xi;Y|Xi\u22121, Xi\u22122, . . . , X 1).\nHint: Write I(X1, X2, . . . , X n;Y) =H(X1, X2, . . . , X n)\u2212H(X1, X2, . . . , X n|Y), and use the chain rule for\nentropy on each term.\n7.The Data Processing Inequality : Random variables X, Y, Z are said to form a Markov chain (denoted by\nX\u2212Y\u2212Z) if\nPZ|XY(z|x, y) =PZ|Y(z|y) for all x, y, z.\nIn other words, the conditional distribution of Zgiven both XandY, depends only on Y.\n(a) Verify that if X\u2212Y\u2212Zis Markov chain, then XandZare conditionally independent given Y, i.e.,\nPXZ|Y(x, z|y) =PX|Y(x|y)PZ|Y(z|y) for all x, y, z.\nUse this to show that Z\u2212Y\u2212Xis also Markov chain whenever X\u2212Y\u2212Zis a Markov chain.\n(b) Prove the following data processing inequality: If X\u2212Y\u2212Zis a Markov chain, then\nI(X;Y)\u2265I(X;Z).\nHint: Consider I(X;Y, Z) and expand in two different ways using the chain rule for information.\nComment : The data processing inequality is useful in many estimation problems. E.g., let Ybe a noisy\nversion of X, and Z=g(Y) be an estimate of Xbased on observing only Y. The inequality says that\nfunctions of the data Ycannot increase the information about X.\n8.Erasure Channel : Compute the capacity of a binary erasure channel with erasure probability \u03b5. (The\ndefinition and transition probabilities of a binary erasure channel are given in Handout 6.)\nX= 01\u2212p\np\nX= 11\u2212pp01\u2212p\nY= 0\np\n11\u2212pY= 1p\n9.Cascade Channel : Consider the cascade of two independent binary symmetric channels shown above, each\nwith crossover probability p <1\n2.\n(a) What is the capacity of the above cascade channel?\n2(b) Show that the capacity of a cascade of mindependent BSCs, each with crossover probability p, is given\nby\n1\u2212H2\u00001\n2(1\u2212(1\u22122p)m\u0001\nwhere H2(x) =\u2212xlogx\u2212(1\u2212x) log(1 \u2212x) is the binary entropy function. Observe that the capacity\nmonotonically decreases as mincreases and tends to 0. Is this consistent with what you\u2019d expect?\n(Hint: Use induction on mto compute the overall crossover probability.)\n10.Z-channel : The Z-channel has binary input and output alphabets and transition probabilities PY|Xgiven\nby the following matrix:\nY\nPY|X0 1\nX0 1 0\n11\n21\n2\nFind the capacity of the Z-channel and the maximising input distribution. (Sketching the input-output\nrelationship for this channel should explain its name.)\nHint: You may need the derivative of the binary entropy function:dH2(\u03b1)\nd\u03b1= log2(1\u2212\u03b1)\n\u03b1for\u03b1\u2208(0,1).\n11.Symmetric channels : Consider a channel PY|Xwith input alphabet Xand output alphabet Y. Recall that\neach row of the transition probability matrix specifies the distribution PY|X=xfor an input symbol x\u2208 X.\nThe channel is said to be symmetric (or more precisely, weakly symmetric) if the rows of the transition\nprobability matrix are all permutations of one another and all the column sums are equal. For example,\nthe channel with transition probability matrix\nPY|X:=\uf8ee\n\uf8f00.3 0.2 0.5\n0.5 0.3 0.2\n0.2 0.5 0.3\uf8f9\n\uf8fb (1)\nis symmetric.\n(a) Show that the capacity of a symmetric channel is C= log|Y| \u2212 H(r), with the uniform distribution\noverXmaximising the mutual information. Here H(r) is the entropy corresponding to a row of the\ntransition probability matrix.\nHint: Start with the following steps (providing justifications):\nI(X;Y) =H(Y)\u2212H(Y|X)\n=H(Y)\u2212H(r)\n\u2264log|Y| \u2212 H(r).\n(b) Compute the capacity of the channel in (1).\n12.Joint typicality : Let X\u2208 {0,1}andY\u2208 {a, b, c}be jointly distributed random variables with the following\njoint probability mass function (pmf) PXY:\nY\nPXY a b c\nX0 01\n41\n4\n11\n41\n40\nLetPXandPYdenote the marginal distributions of XandY, respectively. Define three sets of length- n\nsequences as follows:\nTX\nn=\u001a\nxn:\u22121\nnlog2PX(xn) =H(X)\u001b\n, (2)\nTY\nn=\u001a\nyn:\u22121\nnlog2PY(yn) =H(Y)\u001b\n, (3)\nTXY\nn=\u001a\n(xn, yn) :\u22121\nnlog2PXY(xn, yn) =H(X, Y )\u001b\n. (4)\nHere PX(xn) =Qn\ni=1PX(xi),PY(yn) =Qn\ni=1PY(yi), and PXY(xn, yn) =Qn\ni=1PXY(xi, yi).\n[Note that the conditions (2), (3) and (4) together define the jointly typical set in Handout 7 (p.7) A\u03f5,n\nwith \u03f5= 0.]\n3(a) Let n1adenote the number of occurrences of the symbol pair ( X= 1, Y=a) in ( xn, yn), and define\nn0b, n1b, n0csimilarly. Show that\nTXY\nn=\u001a\n(xn, yn) :n1a+n0b+n1b+n0c\nn= 1\u001b\n.\n(b) Show that TX\nn=\b\nxn:n0+n1\nn= 1\t\n, where n0, n1denote the number of occurrences of 0 ,1, respectively,\ninxn.\n(c) Show that\nTY\nn=\u001a\nyn:2na+nb+ 2nc\nn=3\n2\u001b\n=\u001a\nyn:na+nc\nn=1\n2,nb\nn=1\n2\u001b\n.\n(d) For n= 8, give an example of a sequence pair ( xn, yn) such that ( xn, yn)\u2208 TXY\nn, but yn/\u2208 TY\nn.\n(This shows that all three conditions need to be satisfied for a sequence pair ( xn, yn) to be in the jointly\ntypical set.)\nAnswers to Selected Questions\n2. a) D(P\u2225Q) =1\n2 ln 2(\u00b51\u2212\u00b52)2bits; b) LLR (X1, . . . , X n) =1\n2 ln 2\u0000\n\u00b52\n1\u2212\u00b52\n2+ 2(\u00b52\u2212\u00b51)1\nnP\niXi\u0001\n.\n8. Binary erasure channel capacity = (1 \u2212\u03b5) bits/transmission\n10. Capacity of the Z-channel = 0 .322 bits/transmission. The maximising distribution is PX(0) =3\n5, PX(1) =2\n5.\n11. (b) 0.0995 bits/transmission.\n4",
        "user_comment": "I think this module is a bit difficult. However, the lectuerer did a very good job in introducing the concepts, so I feel I understand them relatively well. This seems to include quiet some math and some concepts like AEP are a bit abstract and unfamiliar."
    },
    {
        "assignment_name": "3F8 FTR",
        "DDL": "1/1",
        "content": "3F8 Inference: Full Technical Report (FTR)\nJos\u00e9 Miguel Hern\u00e1ndez-Lobato & Richard E. Turner\nDue: 4pm on Wed 20 March 2024\nIn this assignment you will implement a Bayesian binary classi\ufb01er. For this, you will apply the Laplace\napproximation to the Logistic Classi\ufb01cation model from the coursework exercise, evaluating the results\nobtained using several metrics, such as the average test log-likelihood and the confusion matrices that were\nalready described in the coursework exercise (see the attached notes on Bayesian classi\ufb01cation and sections\n4.4 and 4.5 from Bishop\u2019s book for a description of the Laplace approximation, note that there is a typo in\nBishop\u2019s book, in equation (4.143), whereS Non the left-hand-side should beS\u22121\nN). You will also investigate\nthe performance of selecting hyper-parameter values by optimizing the model evidence (the normalization\nconstant in Bayes rule). Your answers should contain an explanation of what you do and where code is\nasked for you need only include the central commands (complete listings are unnecessary). You must also\ngive an interpretation of what the numerical values and plots you provide mean. Why are the results the\nway they are? Hand in a maximum of 10 pages.\nYour report should follow the TEMPLATE provided in the moodle website.\na) Describe how to apply the Laplace approximation to the Logistic Classi\ufb01cation model from the course-\nwork exercise. Assume a Gaussian prior over the model parameters with zero mean and variance\n\u03c32\n0:p(\u03b2 m) =1\nZexp(\u22121\n2\u03c32\n0\u03b22\nm)form=0 . . .N. To approximate the predictive distribution, use the\nmethod described in section 4.5.2 of Bishop\u2019s book. Describe the approximation of the model evidence\n(normalization constant in Bayes rule) given by the Laplace approximation.\nb) Write python code for computing the Laplace approximation. Your report should include code for\ncomputing the predictive distribution and the approximation of the model evidence. To obtain the\nMAP solution, use the python functionscipy.optimize.fmin_l_bfgs_b, which performs gradient-\nbased MINIMIZATION (not maximization). Compute the gradients as in the coursework exercise,\nbut including now an additional term for the prior.\nc) Run your code on the data from the coursework exercise after expanding the inputs with radial basis\nfunctions, as you already did in the lab, and\ufb01xing\u03c32\n0=1 andl=0.1. Visualise the predictions\nby adding probability contours to the plots made in exercise (c) from the coursework assignment.\nVisualize also the predictions of the MAP solution obtained withscipy.optimize.fmin_l_bfgs_b.\nHow do the results of the full Bayesian approach di\ufb00er from those of the MAP solution?\nd) Report the\ufb01nal training and test log-likelihoods per datapoint and the 2\u00d72 confusion matrices for\nboth the Laplace approximation and the MAP solution methods. Explain your\ufb01ndings.\ne) Write code to tune\u03c32\n0andlby optimizing the approximation of the model evidence given by the\nLaplace approximation. Use a grid search approach for the optimization process: create a grid of size\n10\u00d710 representing all possible combinations of 10 di\ufb00erent values for\u03c32\n0and 10 di\ufb00erent values for\nland then compute the approximation of the model evidence for each point in this grid. What 10\ndi\ufb00erent points did you choose for\u03c32\n0andl?\nA heat map plot is a graphical representation of data where the individual values contained in a\nmatrix are represented as colors. Visualise a heat map plot with the approximation of the model\nevidence obtained for each value of\u03c32\n0andl.\nWhat are the best values for\u03c32\n0andl?\nf) Report the visualisation of the predictions, the average training and test-loglikelihoods and the 2\u00d72\nconfusion matrices obtained after tuning\u03c32\n0andl. How do these results compare with the ones\nobtained with\u03c32\n0=1 andl=0.1?Due: 4pm on the first Wednesday after the end of Lent term",
        "user_comment": "\"This is a full technical report that we have to write. I haven't looked at it, but my friends said this is quiet time consuming and not very easy.\""
    },
    {
        "assignment_name": "AI Agent Hackathon",
        "DDL": "1/1",
        "content": "CUES x CUCATS AI Hackathon Rulebook \u203b Photographs may be taken during the event and could be published publicly.       If you do not wish to be included, please inform a staff member.  1. Event Details \u25cf Hackathon Name: CUES x CUCATS AI Hackathon \u25cf Theme: AI Agent Application \u25cf Focus: Build an AI agent application to solve a specific pain point in productivity. \u25cf Participants: 60 (~16 teams) Cambridge students. \u25cf Location: Intel Lab (Computer Science Department) \u25cf Date: 8th March - 9th March \u25cf Timetable: \u25cb 8 Mar 11:00\u201312:00 \u2013 Hackathon launch and intro brief \u25cb 8 Mar 12:00\u20139 Mar 18:00 \u2013 Coding, working on the project \u25cb 9 Mar 18:00 \u2013 Project submission \u25cb 9 Mar 18:00\u201319:00 \u2013 Dinner \u25cb 9 Mar 19:00\u201321:00 \u2013 Presentation & Winner Announcement \u25cf Meals Schedule: \u25cb 8 Mar 13:00\u201314:00 \u2013 Lunch \u25cb 8 Mar 19:00\u201320:00 \u2013 Dinner \u25cb 9 Mar (morning) \u2013 Breakfast + Snacks \u25cb 9 Mar 12:00\u201313:00 \u2013 Lunch \u25cb 9 Mar 18:00\u201319:00 \u2013 Dinner   2. Hackathon Challenge \u25cf Title: AI Agents for Productivity \u25cf Problem Statement: \u25cb Individuals and teams across various sectors often struggle with overwhelming workloads, disorganised tasks, and insufficient personalised support. The challenge is to create an AI agent that optimises productivity, in a specific area/task \u25cf Suggested Ideas to get you started: 1. Automated Task & Resource Management: \u25cb Streamline task prioritisation, scheduling, and resource allocation. \u25cb Provide intelligent reminders and updates to keep projects on track. 2. Personalized Insights & Progress Tracking: \u25cb Identify gaps in productivity or skill areas. \u25cb Offer tailored recommendations based on user goals, performance data, and past behavior. 3. Multi-Platform Integration: \u25cb Seamlessly connect with common productivity tools (e.g., email, project management software, collaboration platforms). \u25cb Consolidate notifications, files, and data from various sources into a single interface. 4. Proactive Assistance (Beyond a Simple Chatbot): \u25cb Automate repetitive tasks, such as scheduling meetings or organizing files. \u25cb Provide context-aware suggestions and actions to help users stay organised and focused. \u25cf Fairness Rules: \u25cb Teams can use LLMs (Large Language Models) to generate datasets but cannot use them to generate project ideas. \u25cb LLMs may be used only for small code snippets or assistance with debugging and optimisation. Generating entire modules or full features from an LLM is NOT allowed. \u25cb All code must be written during the hackathon. Pre-existing code libraries can be used only if publicly available. \u25cb Each team must document their development process to verify originality. \u25cb Teams must submit their project source code and a short presentation. 3. Judging Criteria Projects will be evaluated based on the following: 1. Practical Impact (20%) \u25cf How well does the project address real, meaningful problems or productivity challenges within its domain? 2. AI Implementation (20%) \u25cf The effective and appropriate use of AI techniques (e.g., NLP, recommendation systems, automation) while following the hackathon\u2019s fairness rules. 3. Scalability & Industry Potential (20%) \u25cf Could this solution be extended or adapted to help the majority of people, rather than serving just a niche audience? 4. User Experience & Design (20%) \u25cf The intuitiveness, visual appeal, and overall design quality of the solution. 5. Innovation & Creativity (20%) \u25cf How unique, inventive, and groundbreaking is the team\u2019s approach?   4. Additional Rules & Guidelines \u25cf Team Size: max 5 participants per team. \u25cf Submission Format: Source code + 3-minute presentation/demo. \u25cf API Usage: Your agent application must run solely on OpenAI\u2019s API and all your API keys must be stored in .env file. It is important to include what APIs you have used in your project in your documentation. Please note that the event organisers will only provide funding for API usage up to \u00a318 per team. Any costs exceeding this limit will not be covered. \u25cf Intellectual Property: Teams retain ownership of their projects but must allow organisers to showcase winning solutions. \u25cf Conduct: Participants must adhere to ethical AI principles and respectful collaboration.",
        "user_comment": "This is an event I want to participate in. I know some ML and AI, but not very familiar with Agentic AI. "
    },
    {
        "assignment_name": "3F8 EP3",
        "DDL": "1/1",
        "content": "Engineering Tripos Part IIA THIRD YEAR\nPaper 3F8: Inference\nExample Sheet 3: Sequence Modelling and Monte Carlo Methods\nStraightforward questions are marked y\nTripos standard (but not necessarily Tripos length) questions are marked\u0003\nMarkov Models\n1. Markov Models: \ftting bi-gram models\nA data scientist observes part of a long sequence that contains K= 3 charac-\nters:ABAAABBABCCCBC . She would like to use a bi-gram model to \ft\nthe data with parameters p(y1=kj\u0012) =\u00190\nkandp(yt=kjyt\u00001=l;\u0012) =Tk;l.\n(a) Write down the log-likelihood for the model and optimise it with respect\nto\u00190andTto \fnd the maximum likelihood parameter estimates.\n(b) Is the maximum-likelihood estimate sensible? How might you improve\nthe estimate?\n2. Markov Models: Gaussian AR(1) models\nA data scientist observes a sequence of scalar variables y1:T=fytgT\nt=1generated\nfrom a Gaussian AR(1) process yt=\u0015yt\u00001+\u000ftwhere\u000ft\u0018N(\u0016;\u001b2).\nShe knows that the invariant distribution of the process has the following\nproperties\nlim\nt!1E(yt) =\u00161;lim\nt!1E(y2\nt) =\u001b2\n1+\u00162\n1;lim\nt!1E(ytyt\u00001) =\u000b1:\n(a) Derive the parameters of the Gaussian AR(1) process f\u0015;\u0016;\u001b2gin terms\nof the properties of the invariant distribution f\u00161;\u001b2\n1;\u000b1g.\n(b) The data scientist reinterprets the original Markov model in terms of a\nnew random variable ztsuch thatyt=zt+\u00161. State the form of the\ndistribution p(z1:T) required for this model to be equivalent to the original\none.\n1Hidden Markov Models\n3. Discrete Valued Hidden Markov Models\u0003\n(a) Provide the probabilistic equations that de\fne a Hidden Markov Model\n(HMM) for observed data that takes discrete values. Indicate what as-\npects of the model the following terms refer to: initial state probabilities ,\ntransition matrix andemission matrix .\n(b) Consider a dataset consisting of the following string of 160 symbols from\nthe alphabetfA;B;Cg:\nAABBBACABBBACAAAAAAAAABBBACAAAAABACAAAAAA\nBBBBACAAAAAAAAAAAABACABACAABBACAAABBBBACA\nAABACAAAABACAABACAAABBACAAAABBBBACABBACAA\nAAAABACABACAAABACAABBBACAAAABACABBACA\nCarefully analyse the string. Describe an HMM model for the string.\nYour description should include the number of states in the HMM, the\ntransition matrix including the values of the elements of the matrix, the\nemission matrix including the values of its elements, and the initial state\nprobabilities. Explain your reasoning.\n24. Probabilistic Modelling using HMMs for continuous valued observations\n(a) A machine learner observes the time-series, yt, shown below:\nt0 50 100 150 200yt\n-3-2-10123\nSuggest a suitable Hidden Markov Model (HMM) for this sequence and\nstate the model's probabilistic equations. Indicate plausible numerical\nvalues for the parameters where possible.\n(b) The machine learner is provided with a second set of observations ztthat\nwere measured simultaneously with yt, shown below:\nt0 50 100 150 200zt\n-3-2-10123\nExtend the HMM you proposed for part (a) so that it can jointly model\nthe \frst and second set of observations.\n35. Inference in HMMs with Discrete Hidden Statesy\nA Hidden Markov Model contains a discrete hidden state variable xtthat takes\none of two values and a discrete observed state ytthat also takes one of two\nvalues. The hidden state has a transition probability,\n\u0014P(xt= 1jxt\u00001= 1)P(xt= 1jxt\u00001= 2)\nP(xt= 2jxt\u00001= 1)P(xt= 2jxt\u00001= 2)\u0015\n=\u0014T11T12\nT21T22\u0015\n=\u00142=3 1=3\n1=3 2=3\u0015\n:\nThe \fltering distribution at time t\u00001 is\nP(xt\u00001jy1:t\u00001) =\u0014P(xt\u00001= 1jy1:t\u00001)\nP(xt\u00001= 2jy1:t\u00001)\u0015\n=\u00141=4\n3=4\u0015\n:\n(a) Compute the predictive distribution for the next hidden state variable,\nP(xtjy1:t\u00001).\n(b) Explain how your solution to part (a) can be used to compute the \fltering\ndistribution P(xtjy1:t). What additional piece of information would you\nrequire to carry out this computation?\n6. Forecasting in Linear Gaussian State Space Models\u0003\nA simple linear Gaussian state space model with scalar hidden state variables\nxthas been used to model scalar observations yt,\np(xtjxt\u00001;\u0015;\u001b2) =N(xt;\u0015xt\u00001;\u001b2); p(ytjxt;\u001b2\ny) =N(yt;xt;\u001b2\ny):\nThe Kalman \flter recursions have been used to process Tobservations, y1:T, in\norder to return the posterior distribution over the Tth latent state, p(xTjy1:T) =\nN(xT;\u0016T;\u001b2\nT).\n(a) Explain how to transform the posterior distribution over the Tth latent\nstate into a forecast for the observations one time step into the future,\ni.e. express p(yT+1jy1:T) in terms of \u0016Tand\u001b2\nT.\n(b) Now provide a forecast for the observations \u001ctime steps into the future\nby expressing p(yT+\u001cjy1:T) in terms of \u0016Tand\u001b2\nT.\n(c) What happens to p(yT+\u001cjy1:T) as\u001c!1 ?\n4Selected solutions and hints\n1. a) logp(y1:Tj\u0012) = log\u00190\n1+ 2 log(T11T12T32T33) + 3 logT21+ log(T22T23)\nT=2\n42=5 2=5 0\n3=5 1=5 1=3\n0 2=5 2=33\n5; \u00190=2\n41\n0\n03\n5\n2. a)\u0015= (\u000b1\u0000\u00162\n1)=\u001b2\n1,\u0016=\u00161(1+(\u00162\n1\u0000\u000b1)=\u001b2\n1),\u001b2=\u001b2\n1\u0012\n1\u0000\u0010\n\u000b1\u0000\u00162\n1\n\u001b21\u00112\u0013\n3. b) pay close attention to repeated patterns and remember that some parts of\na HMM can be deterministic\n4. b) consider whether the low variance ztregions are correlated through time\nand whether a standard HMM could model this\n5. a)p(xtjy1:t\u00001) = [5;7]>=12\n6. b)p(yT+\u001cjy1:T) =N(yT+\u001c;\u0015\u001c\u0016T;\u001b2\ny+\u00152\u001c\u001b2\nT+\u001b2P\u001c\u00001\nt0=0\u00152t0)\nRichard E. Turner\n5",
        "user_comment": "This module is quiet hard and especially the math behind it. It requires some revision, i guess, before I can fully grasp all the concepts coherently. "
    }
]